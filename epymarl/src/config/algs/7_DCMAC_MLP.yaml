######
#
# MAPPO for DCMAC with MLP
#
# How:
# * Use CV critic (with parameter sharing): ./src/modules/critics/cv.py
#   => INPUT:  (n_parallel_envs, n_timesteps, n_agents, (n_obs x n_agents + one-hot agent ID))
#   => INTER1: (n_parallel_envs, n_timesteps, n_agents, hidden_dim)
#   => INTER2: (n_parallel_envs, n_timesteps, n_agents, hidden_dim)
#   => OUTPUT: (n_parallel_envs, n_timesteps, n_agents, 1)
# * Use CTCE Actor: ./src/modules/agents/ctce_agent.py
#   => INPUT:  (n_parallel_envs, (n_nodes x n_obs))
#   => INTER1: (n_parallel_envs, (n_nodes x n_actions))
#   => INTER2: (n_parallel_envs, (n_nodes x n_actions))
#   => OUTPUT: (n_parallel_envs, n_nodes, n_actions)
#
# * The observations of all agents go throught the layers together
#      in the CTCE agent. One pass for all agents.
# * The critic also merges the observations of all agents
#      but adds the agent ID as a one-hot encoding.
#      This implies one pass per agent.
######


# --- pymarl options ---
runner: "parallel"          
mac: "basic_mac"            
batch_size_run: 2           
test_nepisode: 4        
test_interval: 1000         
test_greedy: True           
log_interval: 100         
runner_log_interval: 100  
learner_log_interval: 100  
t_max: 5000                 
use_cuda: False             
buffer_cpu_only: True       

# --- Logging options ---
use_tensorboard: False      
save_model: False           
save_model_interval: 500000 
checkpoint_path: ""         
evaluate: False             
render: False               
load_step: 0                
save_replay: False          
local_results_path: "epymarl/results"

# --- MAPPO specific parameters ---
action_selector: "soft_policies"
mask_before_softmax: True
buffer_size: 4
batch_size: 4
target_update_interval_or_tau: 200
lr: 0.00013

# --- agent specific parameters ---
obs_agent_id: False
obs_last_action: False
obs_individual_obs: False

# --- Experiment running params ---
agent_output_type: "pi_logits"
learner: "ppo_learner"
entropy_coef: 0.01
use_rnn: False
standardise_returns: True
standardise_rewards: True
q_nstep: 6
critic_type: "cv_critic"
agent: "ctce"
epochs: 4
eps_clip: 0.2
name: "7_DCMAC_MLP"

hidden_dim: 16