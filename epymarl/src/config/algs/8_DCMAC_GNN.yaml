######
#
# MAPPO for DCMAC with GNN
#
# How:
# * Use critic gnn: ./src/modules/critics/critic_gnn.py
#   => INPUT:  (n_parallel_envs, n_timesteps, n_agents, n_obs)
#   => GNN:    (n_parallel_envs, n_timesteps, n_agents, hidden_dim_gnn)
#   => concat: (n_parallel_envs, n_timesteps, n_agents, hidden_dim_gnn + n_obs)
#   => INTER1: (n_parallel_envs, n_timesteps, n_agents, hidden_dim)
#   => INTER2: (n_parallel_envs, n_timesteps, n_agents, hidden_dim)
#   => OUTPUT: (n_parallel_envs, n_timesteps, n_agents, 1)
# * Use Actor gnn: ./src/modules/agents/actor_gnn.py
#   => INPUT:  (n_parallel_envs x n_nodes, n_obs)
#   => GNN:    (n_parallel_envs x n_nodes, hidden_dim_gnn)
#   => concat: (n_parallel_envs x n_nodes, hidden_dim_gnn + n_obs)
#   => INTER1: (n_parallel_envs x n_nodes, hidden_dim)
#   => INTER2: (n_parallel_envs x n_nodes, hidden_dim)
#   => OUTPUT: (n_parallel_envs x n_nodes, n_actions)
#
# * Both the critic and the actor use a GNN to process the observations.
# * The observations of all agents go throught the layers together
#      in the CTCE agent. One pass for all agents.
######

# --- pymarl options ---
runner: "parallel"          
mac: "basic_mac"            
batch_size_run: 2           
test_nepisode: 4        
test_interval: 1000         
test_greedy: True           
log_interval: 100         
runner_log_interval: 100  
learner_log_interval: 100  
t_max: 5000                 
use_cuda: False             
buffer_cpu_only: True       

# --- Logging options ---
use_tensorboard: False      
save_model: False           
save_model_interval: 500000 
checkpoint_path: ""         
evaluate: False             
render: False               
load_step: 0                
save_replay: False          
local_results_path: "epymarl/results"

# --- MAPPO specific parameters ---
action_selector: "soft_policies"
mask_before_softmax: True
buffer_size: 4
batch_size: 4
target_update_interval_or_tau: 200
lr: 0.00013

# --- Agent parameters ---
agent: "actor_gnn"
hidden_dim: 8
obs_agent_id: False
obs_last_action: False
obs_individual_obs: False

# --- Experiment running params ---
name: "8_DCMAC_GNN"
action_selector: "soft_policies"
mask_before_softmax: True
target_update_interval_or_tau: 0.01
obs_individual_obs: False
agent_output_type: "pi_logits"
learner: "ppo_learner"
entropy_coef: 0.01
standardise_returns: True
standardise_rewards: True
use_rnn: False
q_nstep: 6
critic_type: "critic_gnn"
epochs: 4
eps_clip: 0.2
use_shared_gnn: False       # => Use a distinct GNN for critic and agent

use_graph_input: True       # => Concatenate observation with graph specific features
use_6_graph_input: False    # => Use all of these features, not only the 6 most important ones
hidden_dim_gnn: 24
n_layers_gnn: 4