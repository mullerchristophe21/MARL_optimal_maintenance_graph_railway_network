######
#
# MAPPO for DCMAC with GNN (shared GNN for critic and agent)
#
######

# --- pymarl options ---
runner: "parallel"          
mac: "basic_mac"            
batch_size_run: 2           
test_nepisode: 4        
test_interval: 1000         
test_greedy: True           
log_interval: 100         
runner_log_interval: 100  
learner_log_interval: 100  
t_max: 5000                 
use_cuda: False             
buffer_cpu_only: True       

# --- Logging options ---
use_tensorboard: False      
save_model: False           
save_model_interval: 500000 
checkpoint_path: ""         
evaluate: False             
render: False               
load_step: 0                
save_replay: False          
local_results_path: "epymarl/results"

# --- MAPPO specific parameters ---
action_selector: "soft_policies"
mask_before_softmax: True
buffer_size: 4
batch_size: 4
target_update_interval_or_tau: 200
lr: 0.00013

# --- Agent parameters ---
agent: "actor_gnn"
hidden_dim: 8
obs_agent_id: False
obs_last_action: False
obs_individual_obs: False

# --- Experiment running params ---
name: "11_DCMAC_GNN_SHARED"
action_selector: "soft_policies"
mask_before_softmax: True
target_update_interval_or_tau: 0.01
obs_individual_obs: False
agent_output_type: "pi_logits"
learner: "ppo_learner_shared" # => special ppo to learn critic and agent with same parameters (GNN)
entropy_coef: 0.01
standardise_returns: True
standardise_rewards: True
use_rnn: False
q_nstep: 6
critic_type: "critic_gnn"
epochs: 4
eps_clip: 0.2
use_shared_gnn: True       # => Use the same GNN for critic and agent

use_graph_input: True       # => Concatenate observation with graph specific features
use_6_graph_input: False    # => Use all of these features, not only the 6 most important ones
hidden_dim_gnn: 24
n_layers_gnn: 4