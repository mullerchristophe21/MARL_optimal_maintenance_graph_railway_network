######
#
# Dummy Algorithm: Always take action 1 = Tamping
#
# How:
# * Use dummy RNN actor ./src/modules/agents/actor_dummy.py
# * Arrange the actor to always output action 1
# * Critic unchanged, but useless
#
######


# --- pymarl options ---
runner: "parallel"          
mac: "basic_mac"            
batch_size_run: 2           
test_nepisode: 4        
test_interval: 1000         
test_greedy: True           
log_interval: 100         
runner_log_interval: 100  
learner_log_interval: 100  
t_max: 5000                 
use_cuda: False             
buffer_cpu_only: True       

# --- Logging options ---
use_tensorboard: False      
save_model: False           
save_model_interval: 500000 
checkpoint_path: ""         
evaluate: False             
render: False               
load_step: 0                
save_replay: False          
local_results_path: "epymarl/results"

# --- RL hyperparameters ---
gamma: 0.99
batch_size: 8          
buffer_size: 32        
lr: 0.00007            
optim_alpha: 0.99      
optim_eps: 0.00001     
grad_norm_clip: 4      
add_value_last_step: True

# --- Agent parameters ---
agent: "rnn_dummy"            # => follow dummy RNN Agent
dummy_policy_action: 2        # => always take action 2 (= Tamping)

hidden_dim: 4          
obs_agent_id: False    
obs_last_action: False 

# --- Experiment running params ---
repeat_id: 1
label: "default_label"
hypergroup: null


# --- ALGO specific parameters ---

# --- IPPO specific parameters ---
name: "2_OnlyTamping"
action_selector: "soft_policies"
mask_before_softmax: True
target_update_interval_or_tau: 0.01
obs_individual_obs: False
agent_output_type: "pi_logits"
learner: "ppo_learner"
entropy_coef: 0.01
standardise_returns: True
standardise_rewards: True
use_rnn: False
q_nstep: 6
critic_type: "ac_critic"
epochs: 4
eps_clip: 0.2
use_shared_gnn: False
use_shared_transformer: False
critic_loss_coef: 1
hidden_dim_gnn: 4
n_layers_gnn: 2